{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "data = {'client_id':'d9ZEDNx2a_CjiA','response_type':'code','state':'pranav1234','redirect_uri':'https://www.google.com/','scope':'read'}\n",
    "response = requests.get('https://www.reddit.com/api/v1/authorize',params=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/login/?dest=https%3A%2F%2Fwww.reddit.com%2Fapi%2Fv1%2Fauthorize%3Fclient_id%3Dd9ZEDNx2a_CjiA%26response_type%3Dcode%26state%3Dpranav1234%26redirect_uri%3Dhttps%253A%252F%252Fwww.google.com%252F%26scope%3Dread'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = {'grant_type':'authorization_code','code':'7GSTbYM4RiaxS5-xUg33YqC1nVKrKA','redirect_uri':'https://www.google.com/'}\n",
    "\n",
    "r = requests.post('https://www.reddit.com/api/v1/access_token',data=data_2,auth=('d9ZEDNx2a_CjiA','YCKkVaq1_3ZpFDXmMnOyAV4mdsA'),headers={'User-Agent':'pranavkhurana'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"access_token\": \"378519721510-HcWLwAlXNKT_QeeAOqAy60gGZEMWsw\", \"token_type\": \"bearer\", \"expires_in\": 3600, \"scope\": \"read\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "37\n",
      "100\n",
      "100\n",
      "0\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "header = {'Authorization':'bearer 378519721510-HcWLwAlXNKT_QeeAOqAy60gGZEMWsw','User-Agent':'pranavkhurana'}\n",
    "\n",
    "\n",
    "flair_list = ['Artificial Intelligence','Biotechnology','Business','Crypto','Energy','Hardware','Machine Learning','Nanotech/Materials','Networking/Telecom','Net Neutrality','Politics','Privacy','Robotics/Automation','Security','Social Media','Society','Software','Space','Transportation']\n",
    "\n",
    "post_list_master = []\n",
    "for flair in flair_list:\n",
    "    params = {'limit':100,'q':'flair_name:'+flair}\n",
    "    rq = requests.get('https://oauth.reddit.com/r/technology/search',headers=header,params=params)\n",
    "    data = rq.json()\n",
    "    post_list = data['data']['children']\n",
    "    print(len(post_list))\n",
    "    for j in post_list:\n",
    "        post_list_master.append(j)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pd.Series(post_list_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf.to_csv('post_list_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dict = {'post_text':[],'post_flair':[],'post_additional_url':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in post_list_master:\n",
    "    text = i['data']['title']\n",
    "    flair = i['data']['link_flair_text']\n",
    "    if('url_overridden_by_dest' in i['data']):\n",
    "        add_url = i['data']['url_overridden_by_dest']\n",
    "    else:\n",
    "        add_url = \"\"\n",
    "    post_dict['post_text'].append(text)\n",
    "    post_dict['post_flair'].append(flair)\n",
    "    post_dict['post_additional_url'].append(add_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df = pd.DataFrame(post_dict)\n",
    "\n",
    "df['URL Text'] = \"\"\n",
    "\n",
    "df.iloc[[89,580,1092,1093,1454,1542,1543],-2] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  completed\n",
      "2  completed\n",
      "3  completed\n",
      "4  completed\n",
      "5  completed\n",
      "6  completed\n",
      "7  completed\n",
      "8  completed\n",
      "9  completed\n",
      "10  completed\n",
      "11  completed\n",
      "12  completed\n",
      "13  completed\n",
      "14  completed\n",
      "15  completed\n",
      "16  completed\n",
      "17  completed\n",
      "18  completed\n",
      "19  completed\n",
      "20  completed\n",
      "21  completed\n",
      "22  completed\n",
      "23  completed\n",
      "24  completed\n",
      "25  completed\n",
      "26  completed\n",
      "27  completed\n",
      "28  completed\n",
      "29  completed\n",
      "30  completed\n",
      "31  completed\n",
      "32  completed\n",
      "33  completed\n",
      "34  completed\n",
      "35  completed\n",
      "36  completed\n",
      "37  completed\n",
      "38  completed\n",
      "39  completed\n",
      "40  completed\n",
      "41  completed\n",
      "42  completed\n",
      "43  completed\n",
      "44  completed\n",
      "45  completed\n",
      "46  completed\n",
      "47  completed\n",
      "48  completed\n",
      "49  completed\n",
      "50  completed\n",
      "51  completed\n",
      "52  completed\n",
      "53  completed\n",
      "54  completed\n",
      "55  completed\n",
      "56  completed\n",
      "57  completed\n",
      "58  completed\n",
      "59  completed\n",
      "60  completed\n",
      "61  completed\n",
      "62  completed\n",
      "63  completed\n",
      "64  completed\n",
      "65  completed\n",
      "66  completed\n",
      "67  completed\n",
      "68  completed\n",
      "69  completed\n",
      "70  completed\n",
      "71  completed\n",
      "72  completed\n",
      "73  completed\n",
      "74  completed\n",
      "75  completed\n",
      "76  completed\n",
      "77  completed\n",
      "78  completed\n",
      "79  completed\n",
      "80  completed\n",
      "81  completed\n",
      "82  completed\n",
      "83  completed\n",
      "84  completed\n",
      "85  completed\n",
      "86  completed\n",
      "87  completed\n",
      "88  completed\n",
      "90  completed\n",
      "91  completed\n",
      "92  completed\n",
      "93  completed\n",
      "94  completed\n",
      "95  completed\n",
      "96  completed\n",
      "97  completed\n",
      "98  completed\n",
      "99  completed\n",
      "100  completed\n",
      "101  completed\n",
      "102  completed\n",
      "103  completed\n",
      "104  completed\n",
      "105  completed\n",
      "106  completed\n",
      "107  completed\n",
      "108  completed\n",
      "109  completed\n",
      "110  completed\n",
      "111  completed\n",
      "112  completed\n",
      "113  completed\n",
      "114  completed\n",
      "115  completed\n",
      "116  completed\n",
      "117  completed\n",
      "118  completed\n",
      "119  completed\n",
      "120  completed\n",
      "121  completed\n",
      "122  completed\n",
      "123  completed\n",
      "124  completed\n",
      "125  completed\n",
      "126  completed\n",
      "127  completed\n",
      "128  completed\n",
      "129  completed\n",
      "130  completed\n",
      "131  completed\n",
      "132  completed\n",
      "133  completed\n",
      "134  completed\n",
      "135  completed\n",
      "136  completed\n",
      "137  completed\n",
      "138  completed\n",
      "139  completed\n",
      "140  completed\n",
      "141  completed\n",
      "142  completed\n",
      "143  completed\n",
      "144  completed\n",
      "145  completed\n",
      "146  completed\n",
      "147  completed\n",
      "148  completed\n",
      "149  completed\n",
      "150  completed\n",
      "151  completed\n",
      "152  completed\n",
      "153  completed\n",
      "154  completed\n",
      "155  completed\n",
      "156  completed\n",
      "157  completed\n",
      "158  completed\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    if(df.iloc[i,2] != \"\"):\n",
    "        try:\n",
    "            r = requests.get(df.iloc[i,2])\n",
    "            content = r.content\n",
    "            soup = BeautifulSoup(content)\n",
    "            string = \"\"\n",
    "            time_start = time.time()\n",
    "            para_list = soup.find_all('p')\n",
    "            for k in para_list:\n",
    "                string += k.getText()\n",
    "            df.iloc[i,3] = string\n",
    "            print(i,\" completed\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Technology sub\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
